version: "3.5"

networks:
  ai-docks-net:
    driver: bridge

services:
  # webapp:
  #   build:
  #     context: ./webapp
  #     dockerfile: Dockerfile.dev
  #   environment:
  #     - AUTH_SECRET=${APP_AUTH_SECRET}
  #     - NEXTAUTH_URL=${DOMAIN}
  #   env_file:
  #     - ../.env
  #   tty: true # enable colorized logs
  #   volumes:
  #     - ./webapp/:/usr/src/app/
  #     - hf-cache:/.hf_cache
  #   logging:
  #     options:
  #       max-size: "100m"
  #       max-file: "10"
  #   restart: always
  #   networks:
  #     - ai-docks-net
  #   ports:
  #     - 3333:3000
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://webapp:80/"]
  #     interval: 1m
  #     timeout: 20m
  #     retries: 20

  # ai-docks-service:
  #   volumes:
  #     - ./api:/app/:rw
  #     - ./api/models/emb:/app/models/emb:rw
  #     - ./api/models/rr:/app/models/rr:rw
  #     - ./api/models/llm:/app/models/llm:rw
  #     - ../primsa-ai/training/app/models/:/fine-tunes
  #     - hf-cache:/.hf_cache
  #   deploy:
  #     replicas: 1
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   runtime: nvidia
  #   ipc: host
  #   tty: true # enable colorized logs
  #   env_file:
  #     - ../.env
  #   container_name: ai-docks
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #   ports:
  #     - 8723:80
  #   networks:
  #     - ai-docks-net
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - HF_HOME=/.hf-cache
  #   logging:
  #     options:
  #       max-size: "100m"
  #       max-file: "10"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://ai-docks:80/"]
  #     interval: 1m
  #     timeout: 20m
  #     retries: 20

  vision-controller:
    # depends_on:
    #   - vision-worker
    command:
      [
        "python",
        "-m",
        "llava.serve.controller",
        "--host",
        "0.0.0.0",
        "--port",
        "10000",
      ]
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - hf-cache:/.hf_cache
    restart: always
    runtime: nvidia
    env_file:
      - ../.env
    container_name: vision-controller
    tty: true # enable colorized logs
    build:
      context: ../llava
      dockerfile: Dockerfile.1xA000
    ports:
      - 8046:10000
    networks:
      - ai-docks-net
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    ipc: host
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://vision-controller:10000/"]
      interval: 1m
      timeout: 20m
      retries: 20

  vision-playground:
    depends_on:
      - vision-controller
    command:
      [
        "python",
        "-m",
        "llava.serve.gradio_web_server",
        "--controller",
        "http://vision-controller:10000",
        "--model-list-mode",
        "reload",
        "--port",
        "8888",
      ]
    volumes:
      - hf-cache:/.hf_cache
    restart: always
    runtime: nvidia
    env_file:
      - ../.env
    container_name: vision-playground
    tty: true # enable colorized logs
    build:
      context: ../llava
      dockerfile: Dockerfile.1xA000
    ports:
      - 8045:8888
    networks:
      - ai-docks-net
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    ipc: host
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://vision-playground:8888/"]
      interval: 1m
      timeout: 20m
      retries: 20

  vision-server:
    command:
      [
        "python",
        "-m",
        "sglang.launch_server",
        "--model-path",
        "liuhaotian/llava-v1.6-mistral-7b",
        # "--tokenizer-path",
        # "llava-hf/llava-1.5-7b-hf",
        "--port",
        "30000",
      ]
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - hf-cache:/.hf_cache
    restart: always
    runtime: nvidia
    env_file:
      - ../.env
    container_name: vision-server
    tty: true # enable colorized logs
    build:
      context: ../llava
      dockerfile: Dockerfile.1xA000
    ports:
      - 8047:30000
    networks:
      - ai-docks-net
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    ipc: host
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://vision-server:30000/"]
      interval: 1m
      timeout: 20m
      retries: 20

  vision-sglang_worker:
    depends_on:
      - vision-server
    command:
      [
        "python",
        "-m",
        "llava.serve.sglang_worker",
        "--host",
        "0.0.0.0",
        "--controller",
        "http://vision-controller:10000",
        "--port",
        "40000",
        "--worker",
        "http://vision-worker:40000",
        "--sgl-endpoint",
        "http://vision-server:30000",
      ]
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - hf-cache:/.hf_cache
    restart: always
    runtime: nvidia
    env_file:
      - ../.env
    container_name: vision-sglang_worker
    tty: true # enable colorized logs
    build:
      context: ../llava
      dockerfile: Dockerfile.1xA000
    ports:
      - 8048:40000
    networks:
      - ai-docks-net
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    ipc: host
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://vision-sglang_worker:40000/"]
      interval: 1m
      timeout: 20m
      retries: 20

  vision-worker:
    command:
      [
        "python",
        "-m",
        "llava.serve.model_worker",
        "--host",
        "0.0.0.0",
        "--controller",
        "http://vision-controller:10000",
        "--port",
        "40000",
        "--worker",
        "http://vision-worker:40000",
        "--model-path",
        "liuhaotian/llava-v1.6-mistral-7b",
      ]
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - hf-cache:/.hf_cache
    restart: always
    runtime: nvidia
    env_file:
      - ../.env
    container_name: vision-worker
    tty: true # enable colorized logs
    build:
      context: ../llava
      dockerfile: Dockerfile.1xA000
    ports:
      - 8049:40000
    networks:
      - ai-docks-net
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    ipc: host
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://vision-worker:40000/"]
      interval: 1m
      timeout: 20m
      retries: 20
  # llm-inference:
  #   deploy:
  #     replicas: 0
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   volumes:
  #     - ./api/models/llm:/models/
  #     - hf-cache:/.hf_cache
  #   restart: always
  #   runtime: nvidia
  #   env_file:
  #     - ../.env
  #   # container_name: llm-inference
  #   tty: true # enable colorized logs
  #   build:
  #     context: ./llm-inference
  #   ports:
  #     - 8045:80
  #   networks:
  #     - ai-docks-net
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - TOKENIZERS_PARALLELISM=true
  #   ipc: host
  #   command: [
  #       "--model",
  #       "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  #       # "--revision",
  #       # "gptq-4bit-32g-actorder_True",
  #       # "--dtype",
  #       # "float16",
  #       "--block-size",
  #       "32",
  #       # "--max-model-len=32768",
  #       "--host",
  #       "0.0.0.0",
  #       "--port",
  #       "80",
  #       # "--served-model-name",
  #       # "gpt-3.5-turbo",
  #       # "--swap-space",
  #       # "84",
  #       # "--max-num-seqs",
  #       # "16",
  #       # "--max-paddings",
  #       # "16",
  #       # "--trust-remote-code",
  #       # "--seed",
  #       # "42",
  #       # # "--enforce-eager",
  #       # # "--max-parallel-loading-workers",
  #       # # "7",
  #       # "--gpu-memory-utilization",
  #       # "0.8",
  #       # "--max-num-batched-tokens",
  #       # "20480",
  #     ]
  #   logging:
  #     options:
  #       max-size: "100m"
  #       max-file: "10"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://llm-inference:80/"]
  #     interval: 1m
  #     timeout: 20m
  #     retries: 20

  # vector-store:
  #   tty: true # enable colorized logs
  #   ipc: host
  #   build:
  #     context: ./vector-store
  #     dockerfile: Dockerfile
  #   volumes:
  #     - ./vector-store:/chroma
  #     - index_data:/index_data
  #     - embeddings-data:/chroma/chroma
  #   command: uvicorn chromadb.app:app --reload --workers 32 --host 0.0.0.0 --port 8000
  #   #--log-config log_config.yml
  #   environment:
  #     - ANONYMIZED_TELEMETRY=False
  #     - IS_PERSISTENT=TRUE
  #     - NVIDIA_VISIBLE_DEVICES=all
  #   networks:
  #     - ai-docks-net
  #   env_file:
  #     - ../.env
  #   container_name: vector-store
  #   logging:
  #     options:
  #       max-size: "100m"
  #       max-file: "10"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://chroma:8000/api/v1/heartbeat"]
  #     interval: 1m
  #     timeout: 20m
  #     retries: 20

volumes:
  hf-cache:
    driver: local
  index_data:
    driver: local
  embeddings-data:
    driver: local
