version: "3.5"

networks:
  prisma-ai-dev-net:
    driver: bridge

services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    environment:
      - MONGO_USER=${MONGO_USER}
      - MONGO_PASS=${MONGO_PASS}
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_ROOT_USER}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_ROOT_PASS}
      - AUTH_SECRET=${APP_AUTH_SECRET}
      - NEXTAUTH_URL=${DOMAIN}
    env_file:
      - .env
    tty: true # enable colorized logs
    volumes:
      - ./frontend/:/usr/src/app/
      - hf-cache:/.hf-cache
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    restart: always
    networks:
      - prisma-ai-dev-net
    ports:
      - 3000:3000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://frontend:80/"]
      interval: 1m
      timeout: 20m
      retries: 20

  api:
    volumes:
      - ./training/app/models:/home/notebook-user/api/models:rw
      - ./api/api/app:/home/notebook-user/api
      - ./api/api/ingest:/home/notebook-user/ingest/:rw
      - ./api/api/app/__pycache__:/home/notebook-user/api/__pycache__:rw
      - unstructured-cache:/home/notebook-user/.cache/unstructured/
      - hf-cache:/home/notebook-user/.cache/huggingface
      - paddle-cache:/home/notebook-user/.cache/paddle
      - paddle-cache-2:/home/notebook-user/.paddleocr
      - nltk-cache-data:/root/nltk_data
    restart: always
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - TABLE_OCR=paddle
      - GENERAL_SENTENCES=True
      - OCR_AGENT=paddle
      - ENTIRE_PAGE_OCR=False
      - LANGFUSE_HOST=${LANGFUSE_URL}
      - PIP_VERSION=23.2.1
      - TRANSFORMERS_CACHE=/.cache/huggingface/
      - CUDAToolkit_ROOT=/usr/local/cuda
    runtime: nvidia
    stdin_open: true
    tty: true # enable colorized logs
    env_file:
      - .env
    build:
      context: ./api
      args:
        - TRANSFORMERS_CACHE=/.cache/huggingface/
        - NVIDIA_VISIBLE_DEVICES=all
        - CMAKE_ARGS="-DLLAMA_CUBLAS=on"
        - FORCE_CMAKE=1
    container_name: api
    depends_on:
      - chroma
      - mongodb
      - langfuse-app
      - langfuse-db
      - identify
      # - inference
    ports:
      - 8000:80
    networks:
      - prisma-ai-dev-net
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://api:80/"]
      interval: 1m
      timeout: 20m
      retries: 20

  identify:
    depends_on:
      - mongodb
    volumes:
      - ./identify/app:/app:rw
      - hf-cache:/.hf-cache
      - ./api/api/ingest:/ingest/:rw
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true # enable colorized logs
    env_file:
      - .env
    build:
      context: ./identify
      args:
        - NVIDIA_VISIBLE_DEVICES=all
        - CMAKE_ARGS="-DLLAMA_CUBLAS=on"
        - FORCE_CMAKE=1
    container_name: identify
    ports:
      - 8055:80
    networks:
      - prisma-ai-dev-net
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://identify:80/"]
      interval: 1m
      timeout: 20m
      retries: 20

  extraction:
    volumes:
      - ./extraction/app:/app:rw
      - hf-cache:/.hf-cache
      - ./api/api/ingest:/ingest/:rw
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true # enable colorized logs
    ipc: host
    env_file:
      - .env
    build:
      context: ./extraction
      args:
        - NVIDIA_VISIBLE_DEVICES=all
        - CMAKE_ARGS="-DLLAMA_CUBLAS=on"
        - FORCE_CMAKE=1
    container_name: extraction
    ports:
      - 8004:80
    networks:
      - prisma-ai-dev-net
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OPENAI_BASE_API=http://extraction:80/v1/
      - OPENAI_API_KEY=sk-dummy
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://extraction:80/"]
      interval: 1m
      timeout: 20m
      retries: 20

  redis:
    image: redis:latest
    command: redis-server --requirepass $REDIS_PASS --loglevel verbose
    ports:
      - 6379:6379
    volumes:
      - cache-data:/data
    restart: always
    stdin_open: true
    tty: true # enable colorized logs
    container_name: redis-cache
    networks:
      - prisma-ai-dev-net
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://redis:80/"]
      interval: 1m
      timeout: 20m
      retries: 20
  
  chat-memory-cache:
    image: redis:latest
    command: redis-server --requirepass $REDIS_PASS --loglevel verbose
    # ports:
    #   - 6379:6379
    volumes:
      - chat-memory-cache:/data
    restart: always
    stdin_open: true
    tty: true # enable colorized logs
    container_name: chat-memory-cache
    networks:
      - prisma-ai-dev-net
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://chat-memory:80/"]
      interval: 1m
      timeout: 20m
      retries: 20

  rerankings-1:
    image: ghcr.io/huggingface/text-embeddings-inference:0.6
    pull_policy: always
    volumes:
      - embeddings-inference-data:/data
      - hf-cache:/.hf-cache
    depends_on:
      - vllm
    # ports:
    #   - 8080:8080
    ipc: host
    container_name: rerankings-1
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true # enable colorized logs
    networks:
      - prisma-ai-dev-net
    command: [
        "--model-id",
        "BAAI/bge-reranker-large",
        "--hostname",
        "0.0.0.0",
        "--port",
        "8080",
        "--hf-api-token",
        $HUGGINGFACEHUB_API_TOKEN,
        "--huggingface-hub-cache",
        $HF_HUB_CACHE,
        # "/.hf-cache",
        "--max-batch-tokens",
        "16384",
      ]
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://rerankings-1:8080/"]
      interval: 1m
      timeout: 20m
      retries: 20

  embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:0.6 #ghcr.io/huggingface/text-embeddings-inference:latest-grpc
    depends_on:
      - vllm
    pull_policy: always
    volumes:
      - embeddings-inference-data:/data
      - hf-cache:/.hf-cache
    # ports:
    #   - 8080:8080
    ipc: host
    container_name: embeddings
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true # enable colorized logs
    networks:
      - prisma-ai-dev-net
    command: [
        "--model-id",
        "jinaai/jina-embeddings-v2-base-en",
        "--hostname",
        "0.0.0.0",
        "--port",
        "8080",
        "--hf-api-token",
        $HUGGINGFACEHUB_API_TOKEN,
        "--huggingface-hub-cache",
        $HF_HUB_CACHE,
        # "/.hf-cache",
        "--max-batch-tokens",
        "16384",
      ]
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://embeddings:8080/"]
      interval: 1m
      timeout: 20m
      retries: 20

  training:
    volumes:
      - ./training/app:/app:rw
      - ./api/api/ingest:/ingest/:rw
      - training-hf-cache:/.hf-cache
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    runtime: nvidia
    ipc: host
    tty: true # enable colorized logs
    env_file:
      - .env
    build:
      context: ./training
    container_name: training
    ports:
      - 8069:80
    networks:
      - prisma-ai-dev-net
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://training:80/"]
      interval: 1m
      timeout: 20m
      retries: 20

  mongodb:
    image: mongo:latest
    volumes:
      - app-db-data:/data/db
      - app-db-config:/data/configdb
      - ./mongodb/entrypoint.sh:/docker-entrypoint-initdb.d/entrypoint.sh
    command: [--auth]
    tty: true # enable colorized logs
    ports:
      - 27021:27017
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_ROOT_USER}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_ROOT_PASS}
      - DB_NAMES=${DB_NAMES}
      - MONGO_USER=${MONGO_USER}
      - MONGO_PASS=${MONGO_PASS}
    restart: always
    networks:
      - prisma-ai-dev-net
    logging:
      options:
        max-size: "100m"
        max-file: "10"

  # # supabase:

  # vllm:
  #   deploy:
  #     replicas: 0
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 0
  #             capabilities: [gpu]
  #   volumes:
  #     - ./vllm/app:/app
  #     - hf-cache:/.hf-cache
  #     - ./models/:/models/
  #   restart: always
  #   runtime: nvidia
  #   tty: true # enable colorized logs
  #   env_file:
  #     - .env
  #   build:
  #     context: vllm
  #   # container_name: vllm
  #   ports:
  #     - 8044:80
  #   networks:
  #     - prisma-ai-dev-net
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     # - HUGGING_FACE_HUB_TOKEN=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
  #     - TOKENIZERS_PARALLELISM=true
  #   ipc: host
  #   command:
  #     [
  #       "--model",
  #       "LHC88/SauerkrautLM-Mixtral-8x7B-Instruct-AWQ",
  #       # "--revision",
  #       # "gptq-4bit-32g-actorder_True",
  #       # "--dtype",
  #       # "float16",
  #       "--block-size",
  #       "32",
  #       "--max-model-len=32768",
  #       "--host",
  #       "0.0.0.0",
  #       "--port",
  #       "80",
  #       "--served-model-name",
  #       "gpt-3.5-turbo",
  #       "--swap-space",
  #       "84",
  #       # "--max-num-seqs",
  #       # "16",
  #       # "--max-paddings",
  #       # "16",
  #       "--trust-remote-code",
  #       "--seed",
  #       "42",
  #       # "--enforce-eager",
  #       # "--max-parallel-loading-workers",
  #       # "7",
  #       "--gpu-memory-utilization",
  #       "0.8"
  #       # "--max-num-batched-tokens",
  #       # "20480",
  #     ]
  #   logging:
  #     options:
  #       max-size: "100m"
  #       max-file: "10"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://vllm:80/"]
  #     interval: 1m
  #     timeout: 20m
  #     retries: 20
  


  llm-inference:
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./outlines/app:/app
      - hf-cache:/.hf-cache
      - ./models/:/models/
    restart: always
    runtime: nvidia
    tty: true # enable colorized logs
    env_file:
      - .env
    build:
      context: llm-inference
    ports:
      - 8044:80
    networks:
      - prisma-ai-dev-net
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - TOKENIZERS_PARALLELISM=true
    ipc: host
    command:
      ["--help"]
    logging:
      options:
        max-size: "100m"
        max-file: "10"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://vllm:80/"]
      interval: 1m
      timeout: 20m
      retries: 20
  
  langfuse-app:
    tty: true # enable colorized logs
    build:
      context: ./langfuse
      dockerfile: Dockerfile
      args:
        - DATABASE_URL=postgresql://${LANGFUSE_DB_USER:-postgres}:${LANGFUSE_DB_PW:-postgres}@langfuse-db:5432/${LANGFUSE_DB:-postgres}
        - NEXTAUTH_SECRET=${LANGFUSE_SECRET:-postgres}
        - NEXTAUTH_URL=${LANGFUSE_PUBLIC_URL}
        - SALT=${LANGFUSE_SALT:-mysalt}
    depends_on:
      - langfuse-db
    ports:
      - "3001:3000"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://${LANGFUSE_DB_USER:-postgres}:${LANGFUSE_DB_PW:-postgres}@langfuse-db:5432/${LANGFUSE_DB:-postgres}
      - DIRECT_URL=postgresql://${LANGFUSE_DB_USER:-postgres}:${LANGFUSE_DB_PW:-postgres}@langfuse-db:5432/${LANGFUSE_DB:-postgres}
      - NEXTAUTH_SECRET=${LANGFUSE_SECRET:-postgres}
      - SALT=${LANGFUSE_SALT:-mysalt}
      - NEXTAUTH_URL=${LANGFUSE_PUBLIC_URL}
      - TELEMETRY_ENABLED=${TELEMETRY_ENABLED:-false}
      - NEXT_PUBLIC_SIGN_UP_DISABLED=${NEXT_PUBLIC_SIGN_UP_DISABLED:-false}
      - LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES=${LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES:-false}
    networks:
      - prisma-ai-dev-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://langfuse-app:3000/"]
      interval: 1m
      timeout: 20m
      retries: 20

  langfuse-db:
    image: postgres
    tty: true # enable colorized logsLANGFUSE_URL
    restart: always
    environment:
      - POSTGRES_USER=${LANGFUSE_DB_USER:-postgres}
      - POSTGRES_PASSWORD=${LANGFUSE_DB_PW:-postgres}
      - POSTGRES_DB=${LANGFUSE_DB:-postgres}
    volumes:
      - langfuse_data:/var/lib/postgresql/data
    networks:
      - prisma-ai-dev-net
    # ports:
    #   - 5555:5432
    healthcheck:
      test: ["CMD", "curl", "-f", "http://langfuse-db:5432/"]
      interval: 1m
      timeout: 20m
      retries: 20

    chroma:
        tty: true # enable colorized logs
        build:
          context: ./chroma
          dockerfile: Dockerfile
        volumes:
          - ./chroma:/chroma
          - index_data:/index_data
          - embeddings-data:/chroma/chroma
        command: uvicorn chromadb.app:app --reload --workers 32 --host 0.0.0.0 --port 8000
        #--log-config log_config.yml
        environment:
          - ANONYMIZED_TELEMETRY=False
          - IS_PERSISTENT=TRUE
          - NVIDIA_VISIBLE_DEVICES=all
        runtime: nvidia
        # ports:
        #   - 8989:8000
        networks:
          - prisma-ai-dev-net
        logging:
          options:
            max-size: "100m"
            max-file: "10"
        healthcheck:
          test: ["CMD", "curl", "-f", "http://chroma:8000/api/v1/heartbeat"]
          interval: 1m
          timeout: 20m
          retries: 20

  # dpo-hermes:
  #   runtime: nvidia
  #   deploy:
  #     replicas: 0
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   build:
  #     context: ./dpo-hermes/
  #     dockerfile: .devops/full-cuda.Dockerfile
  #     args:
  #       - CUDA_DOCKER_ARCH=sm_80
  #       - LLAMA_CUDA_DMMV_X=64
  #       - LLAMA_CUDA_MMV_Y=2
  #       - LLAMA_NATIVE=1
  #       - LLAMA_LTO=1
  #       - LLAMA_CUBLAS=1
  #       - NVIDIA_VISIBLE_DEVICES=all
  #   volumes:
  #   tty: true # enable colorized logs
  #   ipc: host
  #   command: [
  #       "sh",
  #       "-c",
  #       # !!!IMPORTANT set server timeout 1 sec below identify POST request timeout
  #       "/app/build/bin/server --host 0.0.0.0 --port 8080 -m /models/sauerkrautlm-mixtral-8x7b-instruct.Q8_0.gguf --parallel 60 --threads 4 -tb 1 --ctx-size 61440 --batch-size 512 --n-gpu-layers 33 -cb --timeout 600", # sauerkrautlm-mixtral-8x7b-instruct.Q4_K_M.gguf # & python3.10 /app/ examples/server/api_like_OAI.py --host 0.0.0.0 # dpo-hermes-11b # 30x = 19200 30720#dolphin-2.6-mixtral-8x7b.Q4_K_M.gguf
  #       # "/app/build/bin/server --host 0.0.0.0 --port 8080 -m /models/dpopenhermes-7b-v2.Q8_0.gguf --parallel 60 --threads 1 -tb 1 --timeout 50 --ctx-size 102400 --batch-size 512 --n-gpu-layers 36 -cb & python3.10 /app/examples/server/api_like_OAI.py --host 0.0.0.0"
  #     ] #  --no-mmap --cont-batching have to be set on A100 VM - --ctx-size / --parallel = model context length per slot --verbose Murali --cont-batching --verbose
  #   restart: always
  #   ports:
  #     #   - 8845:8081
  #     - 8846:8080
  #   networks:
  #     - prisma-ai-dev-net
  #   environment:
  #     - CUDA_DOCKER_ARCH=sm_80
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - LLAMA_CUDA_DMMV_X=164
  #     - LLAMA_CUDA_MMV_Y=2
  #     - LLAMA_CUBLAS=1
  #   env_file:
  #     - .env
  #   logging:
  #     options:
  #       max-size: "100m"
  #       max-file: "10"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://dpo-hermes:8080/readyz"]
  #     interval: 1m
  #     timeout: 20m
  #     retries: 20

  # docks:
  #   build:
  #     context: ./docks
  #     dockerfile: Dockerfile
  #   environment:
  #     - FLOWISE_USERNAME=tabloid
  #     - FLOWISE_PASSWORD=truth
  #   volumes:
  #     - hf-cache:/.hf-cache
  #   restart: always
  #   ports:
  #     - 3002:3000
  #   tty: true # enable colorized logs
  #   logging:
  #     options:
  #       max-size: "100m"
  #       max-file: "10"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://docks:3000/"]
  #     interval: 1m
  #     timeout: 20m
  #     retries: 20
  #   networks:
  #     - prisma-ai-dev-net
volumes:
  langfuse_data:
    driver: local
  training-hf-cache:
    driver: local
  hf-cache:
    driver: local
  paddle-cache:
    driver: local
  paddle-cache-2:
    driver: local
  localai-img:
    driver: local
  index_data:
    driver: local
  embeddings-data:
    driver: local
  ingest-files:
    driver: local
  ingest-data:
    driver: local
  nltk-cache-data:
    driver: local
  app-db-data:
    driver: local
  app-db-config:
    driver: local
  api-notebook-user:
    driver: local
  unstructured-cache:
    driver: local
  embeddings-inference-data:
    driver: local
  cache-data:
    driver: local
  chat-memory-cache:
    driver: local
